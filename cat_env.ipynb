{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMZR9IbQbBh1n8ot+OCTj/I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"uWuP8NZfBcOI"},"outputs":[],"source":["# Paste your entire code here\n","# Then press SHIFT+ENTER\n","# cat_env.py\n","import math\n","import json\n","import random\n","import numpy as np\n","import pandas as pd\n","from gymnasium import Env, spaces\n","from scipy.stats import norm\n","\n","# ----------------------------\n","# Some IRT / EAP Helper Functions\n","# ----------------------------\n","\n","def irt_probability(theta, a, b, c):\n","    \"\"\"\n","    3PL item response function:\n","      P(correct) = c + (1 - c) / [1 + exp(-1.7*a*(theta - b))]\n","    \"\"\"\n","    return c + (1 - c) / (1 + math.exp(-1.7 * a * (theta - b)))\n","\n","def eap_estimate(responses, a_vals, b_vals, c_vals, nqt=31, prior_mean=0, prior_sd=1):\n","    \"\"\"\n","    Grid-based EAP (Expected A Posteriori) for theta estimation.\n","    \"\"\"\n","    thetas = np.linspace(-4, 4, nqt)\n","    prior = norm.pdf(thetas, loc=prior_mean, scale=prior_sd)\n","\n","    likelihood = np.ones(nqt)\n","    for (r, a, b, c) in zip(responses, a_vals, b_vals, c_vals):\n","        # Probability of correct at each grid point\n","        p_grid = c + (1 - c) / (1 + np.exp(-1.7 * a * (thetas - b)))\n","        likelihood *= p_grid**r * (1 - p_grid)**(1 - r)\n","\n","    posterior = likelihood * prior\n","    posterior /= (posterior.sum() + 1e-12)  # avoid zero division\n","\n","    return np.sum(thetas * posterior)\n","\n","def fisher_information(a, b, c, theta):\n","    \"\"\"\n","    Fisher Information for 3PL at a given theta.\n","    \"\"\"\n","    # D constant ~ 1.7^2 = 2.89\n","    D2 = 2.89\n","    num = D2 * (a**2) * (1 - c)\n","    denom1 = c + np.exp(1.7 * a * (theta - b))\n","    denom2 = (1 + np.exp(-1.7 * a * (theta - b)))**2\n","    return num / (denom1 * denom2)\n","\n","def standard_error(a_vals, b_vals, c_vals, theta):\n","    \"\"\"\n","    Standard error of the theta estimate using sum of Fisher Info.\n","    \"\"\"\n","    total_info = 0.0\n","    for (a, b, c) in zip(a_vals, b_vals, c_vals):\n","        total_info += fisher_information(a, b, c, theta)\n","    return 1.0 / np.sqrt(total_info + 1e-12)\n","\n","\n","# ----------------------------\n","# Generate an Item Bank (similar to your code)\n","# ----------------------------\n","def generate_item_bank(num_items=100, seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","\n","    difficulty_values = np.clip(np.random.normal(0, 1, num_items), -3, 3)\n","    discrimination_values = np.clip(np.random.lognormal(0, 0.5, num_items), 0, 3)\n","    guessing_values = np.random.uniform(0.05, 0.3, num_items)\n","    slip_values = np.random.uniform(0.01, 0.3, num_items)\n","\n","    lo_keys = [\"lo1\", \"lo2\", \"lo3\", \"lo4\"]\n","    item_bank = []\n","    for i in range(num_items):\n","        # Random learning objectives\n","        num_lo = random.randint(1, 3)\n","        chosen_los = random.sample(lo_keys, num_lo)\n","        lo_dict = {lo: random.choice([0,1]) for lo in chosen_los}\n","\n","        item_bank.append({\n","            \"item_id\": i+1,\n","            \"difficulty\": round(float(difficulty_values[i]), 3),\n","            \"discrimination\": round(float(discrimination_values[i]), 3),\n","            \"guessing\": round(float(guessing_values[i]), 3),\n","            \"slip\": round(float(slip_values[i]), 3),\n","            \"learning_objectives\": json.dumps(lo_dict)\n","        })\n","\n","    df = pd.DataFrame(item_bank)\n","    return df\n","\n","\n","# ----------------------------\n","# CATEnv: Our custom RL environment\n","# ----------------------------\n","class CATEnv(Env):\n","    \"\"\"\n","    A Gymnasium-style environment for a simple CAT.\n","    Action space: Discrete( num_items ) -> select an item_id from [0..num_items-1].\n","    Observation: [current_theta_estimate] (1D float).\n","    The environment ends when min_items <= #administered and (SE_theta < 0.3 or #administered >= max_items).\n","    Or if we run out of items.\n","    Reward is based on how the updated theta changes, for instance.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 item_bank_df: pd.DataFrame,\n","                 agent_true_ability: float = 0.0,\n","                 min_items: int = 10,\n","                 max_items: int = 30):\n","        super().__init__()\n","\n","        self.item_bank_df = item_bank_df.copy().reset_index(drop=True)\n","        self.num_items = len(self.item_bank_df)\n","\n","        # The agent's *true* ability (used to simulate correct/incorrect).\n","        self.agent_true_ability = agent_true_ability\n","\n","        # Minimum and maximum items\n","        self.min_items = min_items\n","        self.max_items = max_items\n","\n","        # Define action/obs spaces\n","        self.action_space = spaces.Discrete(self.num_items)  # choose item index\n","        self.observation_space = spaces.Box(\n","            low=-4, high=4, shape=(1,), dtype=np.float32\n","        )\n","\n","        self.reset()\n","\n","    def reset(self, seed=None, options=None):\n","        if seed is not None:\n","            self.seed(seed)\n","\n","        self.administered_indices = []\n","        self.responses = []\n","        self.a_vals = []\n","        self.b_vals = []\n","        self.c_vals = []\n","\n","        self.theta_est = 0.0\n","        self.steps = 0\n","\n","        # Return initial observation\n","        return np.array([self.theta_est], dtype=np.float32), {}\n","\n","    def seed(self, seed):\n","        random.seed(seed)\n","        np.random.seed(seed)\n","\n","    def step(self, action):\n","        \"\"\"\n","        action: integer in [0..num_items-1], the index in item_bank_df\n","        we'll administer that item if not used, else random fallback\n","        \"\"\"\n","\n","        # If item was already used, pick a random new one\n","        # (or you can define a penalty for re-using an item).\n","        if action in self.administered_indices:\n","            # Possibly penalize or pick a random new item\n","            valid_actions = [i for i in range(self.num_items) if i not in self.administered_indices]\n","            if len(valid_actions) == 0:\n","                # No more items\n","                done = True\n","                return np.array([self.theta_est], dtype=np.float32), 0.0, done, {}\n","            action = random.choice(valid_actions)\n","\n","        row = self.item_bank_df.iloc[action]\n","        a = float(row[\"discrimination\"])\n","        b = float(row[\"difficulty\"])\n","        c = float(row[\"guessing\"])\n","\n","        # Simulate correct/incorrect with the agent's *true* ability:\n","        p_correct = irt_probability(self.agent_true_ability, a, b, c)\n","        r = 1 if np.random.rand() < p_correct else 0  # 0 or 1\n","\n","        self.administered_indices.append(action)\n","        self.responses.append(r)\n","        self.a_vals.append(a)\n","        self.b_vals.append(b)\n","        self.c_vals.append(c)\n","\n","        # EAP update\n","        old_theta = self.theta_est\n","        self.theta_est = eap_estimate(self.responses, self.a_vals, self.b_vals, self.c_vals)\n","        self.steps += 1\n","\n","        # Stopping rule check\n","        se = standard_error(self.a_vals, self.b_vals, self.c_vals, self.theta_est)\n","        done = False\n","        if self.steps >= self.min_items and (se <= 0.3 or self.steps >= self.max_items):\n","            done = True\n","\n","        # Reward: let's do: difference in theta + (0.5 if correct, else 0)\n","        # This is just an example.\n","        reward = (self.theta_est - old_theta) + (0.5 if r == 1 else 0)\n","\n","        obs = np.array([self.theta_est], dtype=np.float32)\n","        info = {\"p_correct\": p_correct, \"se\": se, \"score\": r}\n","        return obs, float(reward), done, info"]}]}